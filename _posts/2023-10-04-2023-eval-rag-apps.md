---
layout: post
title:  "Streamlining the evaluation of Retrieval Augmented Generation (RAG) apps"
date: 2023-10-04 05:00:00 -0600
---

Does every developer working on an LLM-backed app go through a trough of post-hack-week sorrow? I sure have! Perhaps your app can perform a few mic-drop tasks, but getting it to reliably reproduce those results (or perform well on a broader set of tasks) is a frustrating, non-deterministic nightmare. A public release is perpetually one week away.

Thankfully, I've mostly emerged from the AI trough by implementing a streamlined Eval Driven Development (EDD) flow. This process has given me a clear understanding of where my LLM-backed app stands across a broad set of tasks, guiding which knobs I turn (and how) to improve performance. My EDD flow is less brittle than a traditional test suite, accurately reflects how a human perceives successes and failures, and adapts to changing environments.

In this post, I'll share how I've implemented a streamlined version of EDD. However, before we dive in, we need to talk about the most common reason LLM-backed apps fail to perform consistently. _This_ is where we're going to focus our evaluation efforts to maximize the reward / effort ratio.

## RAG and context

First, I'm assuming your app is doing some form of Retrieval Augmented Generation (RAG). A RAG app fetches up-to-date or semantically-relevant text from an external source and inserts it into an LLM prompt as _context_. The prompt asks the LLM to reference this context when generating a response.

![context_example](/img/posts/rag_eval/context_example.png)

For example, if you are building a "chat with your docs" app, you could load the doc into a vector database, query the database for the snippets that appear most relevant to your question, and insert those snippets as context into the LLM prompt. The LLM then generate a response based on the question and the context you inserted.

While the most common RAG apps fetch semantically-relevant snippets from vector databases, there are many other services you could interact with, parse results, and insert the results as context. For example, you could interact with a weather REST API to get the forecast for a location in JSON format, insert that JSON into a prompt, and let the LLM tell you the weather in natural language:

![api context example](/img/posts/rag_eval/api_context_example.png)

When people build apps that interact with external systems beyond vector databases, they typically refer to these as _autonomous agents_. Autonomous agents - in almost all cases - have a RAG foundation coupled with extra reasoning sauce. Interacting with diverse external systems often requires additional handling of the results (for example: handling HTTP errors, code generation errors, etc).

__So, the common piece of an LLM-backed app (a RAG system or an autonomous agent) is that you _insert context_ into the prompt and ask the LLM to reference it when providing a response.__

## Where LLM-backed apps go wrong: poor context

Now that we've covered the foundation of LLM-backed apps - RAG - let's examine where these apps most frequently go wrong. This is critical to understand which part of the app we need to focus our evaluation efforts on.

Over the past couple of months, I've talked with a number of people who are building LLM-backed apps like me. With fairly uniform consensus, we've found LLMs to be reliable when provided with good context. Here's a [slide](https://drive.google.com/file/d/1dB-RQhZC_Q1iAsHkNNdkqtxxXqYODFYy/view) from Colin Jarvis of OpenAI with a matrix of typical RAG evaluation results:

![moat](/img/posts/rag_eval/moat.png)

Colin shows that context issues are the primary driver of poor results. While there are no guarantees with LLMs, the surest one I've found for getting an inaccurate response: feed the LLM bad context. For example, if you ask for the weather _today_ but you actually insert the forecast for _tomorrow_ into the context, the LLM will not magically change your context and fetch the weather for the correct date.

There are many ways to feed an LLM bad context.

There are many types of context that are likely to trigger an incorrect response, but two of the most common are:

1. __Irrelevant context__ - if your RAG system is injecting poor context into a prompt, the LLM will have a hard time generating a good and consistent response. Poor context covers _many_ categories: unrelated context, too much context, expired results, unexpected context, etc. There are [many ways to tune this](https://medium.com/@imicknl/how-to-improve-your-chatgpt-on-your-data-solution-d1e842d87404).

2. __Code generation inconsistencies__ - autonomous agents that rely on code generation and evaluation to interact with an external system (like a weather forecasting API) and insert the result as context are hit from two sides. First, you aren't guaranteed that the code the LLM returns is valid. Second, the result format may not be understandable when inserted as context. Perhaps the most well-known autonomous agent that relies on code generation and evaluation is ChatGPT's [Advanced Data Analysis](https://dev.to/ppiova/advanced-data-analysis-in-chatgpt-replaces-code-interpreter-3lil) BETA feature. If you try this feature, you'll likely see it generate and evaluate code, encounter errors, self-recover, and parse varying formats of data as context.

So, at this point, if we're speaking in generalities:

1. Inaccurate responses are typically due to poor context.
2. LLMs generally provide correct responses when provided with appropriate context.

Now, let's see how the above can help us evaluate our LLM-backed app.

## Why evaluation?

It's very likely your app is inserting context into a prompt and that then when things go wrong, the provided context is poor. It's also possible that the app handles certain categories of tasks just fine (but still occasionally has issues) and has high failure rates on other categories of tasks. Ad-hoc human evaluation doesn't have enough breadth - and takes too much time - to give you confidence in the app's performance. You are unaware of the impact of adjusting prompts, context, model params, and fine-tuning on a large breadth of questions.

It's time to build an evaluation system.

Building apps on top of LLMs blends traditional software development with classical machine learning, so let's look at how evaluation is used in both of these domains and what we can apply when evaluating LLM-backed apps.

### Traditional Software - Test Driven Development (TDD)

In traditional software development, there is a process called Test Driven Development (TDD). TDD emphasizes writing tests before writing the actual code. You consider your work "feature complete" when the test suite passes.

In reality, many developers do a mix of writing tests before and after writing code. Regardless, having a test suite with good coverage makes it less risky to refactor code and add new features.

If you were testing an answer using TDD, it might look like this:

```ruby
assert_equal "The weather in Boulder, CO is 70 degrees and sunny", your_model.chat!("What is the weather today in Boulder?")
```
However, this type of static test doesn't work well for LLM-backed apps. The LLM may generate many different correct responses for a given question.

### Classical machine learning

Natural Language Processing (NLP) is a classical machine learning domain and these models have their own evaluation techniques and metrics like BLEU, ROUGE, BERTScore, and MoverScore.

For example, [BLEU is calculated](https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b) by comparing the n-grams in the generated text to the n-grams in the reference text. Here's how you would calculate the 1-gram precision:

![bleu](/img/posts/rag_eval/precision_bleu.webp)

However, there’s poor correlation between these NLP evaluation metrics and human judgments. From Eugene Yan's excellent post _[Patterns for Building LLM-based Systems & Products](https://eugeneyan.com/writing/llm-patterns/)_:

> BLEU, ROUGE, and others have had negative correlation with how humans evaluate fluency. They also showed moderate to less correlation with human adequacy scores. In particular, BLEU and ROUGE have low correlation with tasks that require creativity and diversity.

Additionally, classical machine learning models can leverage large datasets to assist with evaluation. However, the major breakthrough provided by LLMs is they are frequently useful without _any_ additional training (zero-shot) or a very small number of samples (few-shot) so large evaluation datasets are unlikely to exist.

### Accounting for LLM environments

__It's critical that the results we're evaluating are as close to the end user experience as possible.__ For example, if you have a traditional web app that is most frequently used on mobile devices, it's likely your test suite for the mobile experience is more extensive than the desktop version.

LLM-backed apps - especially autonomous agents - are often deployed in environments where the underlying data is changing frequently. My autonomous agent, [OpsTower.ai](https://github.com/opstower-ai/llm-opstower), interacts with AWS to retrieve real-time data about a customer's cloud infrastructure. The data is constantly changing and there are multiple ways to fetch information that can lead to the same result. It's not feasible to build a static test suite that covers all possible scenarios of API interactions.

### Blending TDD and NLP evaluation for LLM-backed apps

So, here's where we're at:

1. A production, traditional software app will have a test suite.
2. These test suites don't work for LLM-backed apps as they can provide many correct responses for a given question.
3. Classical NLP evaluation metrics don't work well for LLM-backed apps.
4. LLM-backed may not have _any_ training data, so we don't have a training set to evaluate against.
5. LLM-backed apps are often deployed in environments where the underlying data is changing frequently.

This means there are 3 evaluation problems we need to solve:

1. Human-like evaluation of LLM responses - our end-user is a human, and we need to evaluate responses similar to how a human perceives them.
2. Eval dataset generation - we need to generate a dataset to evaluate against.
3. Dynamic environments - we need to account for interactions with external systems that are constantly changing.

Let's start with the first problem.

### Solving problem 1 (human-like eval) with model-based evaluation

It's not possible to have a quick code change - eval feedback loop if you are relying on human evaluation. We need another approach.

How well can an LLM fill in as a human evaluator? It turns out, pretty well:

> GPT-4 as an evaluator had a high Spearman correlation with human judgments (0.514), outperforming all previous methods. It also outperformed traditional metrics on aspects such as coherence, consistency, fluency, and relevance. On topical chat, it did better than traditional metrics such as ROUGE-L, BLEU-4, and BERTScore across several criteria such as naturalness, coherence, engagingness, and groundedness.

And:

> Overall, they found that GPT-4 not only provided consistent scores but could also give detailed explanations for those scores. Under the single answer grading paradigm, GPT-4 had higher agreement with humans (85%) than the humans had amongst themselves (81%). This suggests that GPT-4’s judgment aligns closely with the human evaluators.

Personally, I started with a human-evaluation process and when switching to model-based evaluation observed nearly identical scoring:

![human vs model eval](/img/posts/rag_eval/human_vs_model_eval.png)

So, let's go with the assumption that LLMs can be used in place of human evaluation. I'll fill in how this is done later.

### Solving problem 2 (lack of an eval dataset)

Earlier I generalized that most incorrect answers are due to poor context.

... then we can focus our evaluation efforts on automated evaluation of generated context. __Rather than having ground truth natural language answers, we can instead reference a dynamic function known to retrieve the correct context for each question.__ For example, if our dataset is a Pandas dataframe of Titantic passengers like this:

```python
titanic_path = "https://raw.githubusercontent.com/jorisvandenbossche/pandas-tutorial/master/data/titanic.csv"
df = pd.read_csv(titanic_path)
```

...and we want to dynamically validate the number of passengers on the Titanic, we can reference a function that has been verified by a human to return the correct answer:

```
("How many passengers were on the Titanic?", "len(df)"),
```

As of this writing, the closest approach I've seen to this in a product or project is Langsmith's [Dynamic Evaluation](https://github.com/langchain-ai/langsmith-cookbook/blob/main/testing-examples/dynamic-data/testing_dynamic_data.ipynb) (closed beta). However, it's not difficult to implement. I'll show how I've implemented this in my own LLM-backed app.

## Steps to implement dynamic, model-based evaluation of LLM apps

### 1. Generate dataset questions (can use an LLM to assist)

To start, I use ChatGPT to generate a few initial questions for a new evaluation dataset. I'm working on [OpsTower.ai](https://github.com/opstower/llm-opstower), a DevOps AI Assistant, so lets create a dataset of questions about AWS CloudWatch Logs. [Here](https://chat.openai.com/share/8ca6e589-e0f0-4c70-a7a7-e96c0b5d76fb) is my transcript.

![cloudwatch logs](/img/posts/rag_eval/cloudwatch_logs_eval.png)

I then paste these questions in a `aws_cloudwatch_logs.csv` file:

<script src="https://gist.github.com/itsderek23/be29a12b3b565a7918004807e440d387.js"></script>

### 2. Programmatically generate responses for each question

Next I programmatically generate responses to each of these questions using the current AI agent. For my app, it looks like this:

```ruby
demo_source = Eval::VendorTest.new.source.save!
test = AgentTest.create!(source: demo_source, dataset_file: "aws_cloudwatch_logs.csv")
test.run!
```
When the test completes, I view the answers in the UI:

<a href ="/img/posts/rag_eval/aws_cloudwatch_log_results.png" target="_blank"><img src="/img/posts/rag_eval/aws_cloudwatch_logs_results.png"></a>

### 3. Save ground truth functions for generating context

I then review the results. Results generally fall into three buckets:

1. Works as-is - the agent generated valid context and answered the question correctly. I'll reuse the functions that generated the context as ground truth.
2. Hybrid - the agent did not generate valid context, but the code it created to generate context is a solid starting point. I can modify the code then save it as a reference function.
3. Fully human-generated - the agent failed miserably at code generation. I'll write a new function from scratch.

I like to go from easy to hard. The easiest ones are "works as-is" as I can simply copy and save the generated code.

To create a reference function from a "works as-is" result, I generate a saved method from the code the agent generated then reference that function by ID in the `aws_cloudwatch_logs.csv` file.

For example, the question _"How many CloudWatch Log Groups do I have?"_ is correctly answered below:

<a href ="/img/posts/rag_eval/how_many_logs_chat.png" target="_blank"><img src="/img/posts/rag_eval/how_many_logs_chat.png"></a>

I want to save the code the LLM generated, starting with the `get_cloudwatch_log_groups_count` method. In my app, I can do this by executing:

```ruby
saved_methods = SavedMethod.create_from_chat!("482273fb-4c79-4cd2-bc4d-382945c38e42")
saved_methods.map(&:id)
["74294dde-8cd0-4803-a0c2-7c117b8b15de"]
```

I then paste the saved method ID into the `aws_cloudwatch_logs.csv` file:

```csv
question,reference_functions
How many CloudWatch Log Groups do I have?,"74294dde-8cd0-4803-a0c2-7c117b8b15de"
```

I repeat this process for each "works as-is" result. Hybrid and fully human-generated results are handled similarly, but with more changes to the code.

### 4. Run evaluation.

After adding reference functions, I run an evaluation. I can see that in the latest run, the agent incorrectly answered the question _"How many CloudWatch Log Groups do I have?"_:

<a href ="/img/posts/rag_eval/incorrect.png" target="_blank"><img src="/img/posts/rag_eval/incorrect.png"></a>


#### Evaluation Prompts

To perform an evaluation, I leverage two prompts.

##### Prompt 1: Answer from a reference function

The first prompt is used to generate the ground truth answer from the referenced function we saved earlier. It looks like this:

```
You are given a question and function output(s). Return a final answer to the question based on the function output. BE SPECIFIC, sharing data to back up your answer when you can.

Question:: %%QUESTION%%

# Function Output(s)
%%FUNCTION_OUTPUTS%%

Begin!
```

##### Prompt 2: Evaluation Prompt

The second prompt is my model-evaluation prompt. This prompt generates a confidence score, comparing the answer from the agent vs the response generated from the above prompt:

```
I'm comparing the output of two different systems. I need you to compare the two responses to the same question and see if they are functionally equivalent.

It's ok if one response is more detailed than the other, as long as they are functionally equivalent.
It's ok if when comparing floats (ie 10.5 or 12.4%), if the two numbers are within 30% of each other.

Question: %%QUESTION%%

# Response

%%RESPONSE_TO_EVAL%%

# Reference Response

%%REFERENCE_RESPONSE%%

# Response format

Answer in the following format:

{
  "confidence": Return a confidence value btw 0.0-1.0 that the responses are functionally equivalent,
  "why": "A single sentence explaining why you chose this confidence value"
}

Begin!
```

### Rinse and repeat

Once I'm getting acceptable accuracy on an evaluation dataset (ex: 80% or greater), I'll repeat the process outlined above:

1. Add questions with ChatGPT.
2. Add reference functions.
3. Run evaluation, tweak the app, run evaluation, etc.

## Conclusion

I've dramatically increased the accuracy, capabilities, and reliability of my LLM-backed app by leveraging a more automated, streamlined form of Eval Driven Development. My flavor of EDD leans on the LLM to generate the question dataset, uses human-evaluated reference functions to generate context, re-assembles ground truth answers via the LLM, and finally uses the LLM again to simulate human evaluation.

For my app, the resulting confidence scores of this approach are typically within 5% of human-eval scoring at a fraction of the time spent.

### References

To see example datasets, reference functions, and evaluation prompts, checkout the [DevOps AI Assistant Open Leaderboard](https://github.com/opstower-ai/devops-ai-open-leaderboard) on GitHub.

See [Awesome Eval Driven Development](https://github.com/itsderek23/awesome-eval-driven-development) on GitHub a continually updated set of resources related to Eval Driven Development.




