---
layout: post
title:  "Streamlining RAG evaluation"
date: 2023-10-04 05:00:00 -0600
---

![llm product curve](/img/posts/rag_eval/llm_product_curve.png)

My first weeks working with GPT-4 were _magical_. I was doing things that I previously thought were _impossible_. However, as it went from promising proof-of-concept to something I wanted to share, I fell into a trough of sorrow. Every live demo felt like a YOLO moment ... who knew what would happen?

My app could perform a few mic-drop tasks, but getting it to reliably reproduce those results was a frustrating, non-deterministic nightmare. A public release was perpetually one week away.

Today, I'm emerging from my AI trough of sorrow thanks to a streamlined [Eval Driven Development (EDD)](https://eugeneyan.com/writing/llm-patterns/#how-to-apply-evals) flow. This process has given me a clear understanding of where my LLM-backed app stands across a broad set of tasks, guiding which levers I pull (and how far) to improve accuracy. My EDD flow is less brittle than a traditional test suite, is quick to run, accurately reflects how a human perceives successes and failures, and adapts to changing environments.

In this post, I'll share how I've implemented a streamlined version of EDD. However, before we dive in, we need to talk about the most common reason LLM-backed apps fail to perform consistently. _This_ is where I've focused my evaluation efforts to maximize the reward / effort ratio.

## Table of Contents

1. [RAG: the hack for adding knowledge to LLMs](#rag-the-hack-for-adding-knowledge-to-llms)
1. [RAG: like all hacks, it breaks](#rag-like-all-hacks-it-breaks)
1. [EDD speed over accuracy tradeoffs](#edd-speed-over-accuracy-tradeoffs)
1. [Implementing my streamlined EDD flow](#implementing-my-streamlined-edd-flow)
1. [Conclusion](#conclusion)
1. [EDD Resources](#edd-resources)

## RAG: the hack for adding knowledge to LLMs

First, I'm assuming your app is doing some form of [Retrieval Augmented Generation (RAG)](https://www.promptingguide.ai/techniques/rag). A RAG app fetches up-to-date or semantically-relevant text from an external source and inserts it into an LLM prompt as _context_. The prompt asks the LLM to reference this context when generating a response.

![context_example](/img/posts/rag_eval/context_example.png)
<div class="caption">Context is just text. In the example above, I've provided instructions that the model should reference when responding in the OpenAI Playground. You can insert this text programmatically in many ways.</div>

For example, if you are building a "chat with your docs" app, you could load the doc into a vector database, query the database for the snippets that appear most relevant to your question, and insert those snippets as context into the LLM prompt. The LLM then generate a response based on the question and the context you inserted.

While the most common RAG apps fetch semantically-relevant snippets from vector databases, there are many other services you could interact with, parse results, and insert the results as context. For example, you could interact with a weather REST API to get the forecast for a location in JSON format, insert that JSON into a prompt, and let the LLM tell you the weather in natural language:

![api context example](/img/posts/rag_eval/api_context_example.png)
<div class="caption">In this example, I insert the JSON response for the weather forecast as context into the LLM prompt. GPT-4 is able to understand raw data.</div>

When people build apps that interact with external systems beyond vector databases, they typically refer to these as _autonomous agents_. Autonomous agents - in almost all cases - have a RAG foundation coupled with extra reasoning sauce. Interacting with diverse external systems often requires additional handling of the results (for example: handling HTTP errors, code generation errors, etc).

So, __the common piece of an LLM-backed app__ (a RAG system or an autonomous agent) is that you __insert context__ into the prompt and ask the LLM to reference it when providing a response.

## RAG: like all hacks, it breaks

Now that we've covered the foundation of LLM-backed apps - RAG - let's examine where these apps most frequently go wrong. This is critical to understand which part of the app we need to focus our evaluation efforts on.

Here's a [slide](https://drive.google.com/file/d/1dB-RQhZC_Q1iAsHkNNdkqtxxXqYODFYy/view) from Colin Jarvis of OpenAI with a matrix of typical RAG evaluation results:

![moat](/img/posts/rag_eval/moat.png)
<div class="caption">In this example, only 5% of answers are incorrect when the retrieval is correct.</div>

__Colin shows that context issues are the primary driver of poor results.__ While there are no guarantees with LLMs, the surest one I've found for getting an inaccurate response: feed the LLM bad context. For example, if you ask for the weather _today_ but you actually insert the forecast for _tomorrow_ into the context, the LLM will not magically change your context and fetch the weather for the correct date. The LLM is reliable when provided with good context.

_My experience of poor context being the major driver of poor accuracy echos Colin's._

How hard can it be to dump context into a prompt correctly? Well, it turns out there are many ways to feed an LLM bad context.

### Common causes of poor context

There are many types of context that are likely to trigger an incorrect response, but some of the most common are:

#### 1. Irrelevant context

If your RAG system is injecting poor context into a prompt, the LLM will have a hard time generating a good and consistent response. Poor context covers _many_ categories: unrelated context, too much context, expired results, unexpected context, etc. There are [many ways to tune this](https://medium.com/@imicknl/how-to-improve-your-chatgpt-on-your-data-solution-d1e842d87404).

#### 2. Code generation inconsistencies

![code generation](/img/posts/rag_eval/code_int.png)
<div class="caption">ChatGPT's Advanced Data Analysis extension generates code, evaluates it, and returns a natural language answer based on the code result. Agents like this generate widely different context that can trigger unexpected results.</div>

Autonomous agents that rely on code generation and evaluation to interact with an external system (like a weather forecasting API) typically insert the result as context. These apps are hit from two sides. First - unlike querying a Vector DB to insert context - you aren't reasonably guaranteed that the code the LLM generated will execute without error. Second, the result format may not be understandable when inserted as context. Perhaps the most well-known autonomous agent that relies on code generation and evaluation is ChatGPT's [Advanced Data Analysis](https://dev.to/ppiova/advanced-data-analysis-in-chatgpt-replaces-code-interpreter-3lil) BETA feature. If you try this feature, you'll likely see it generate and evaluate code, encounter errors, self-recover, and parse varying formats of data as context.

#### 3. Working around the context window limit

If you wanted GPT-4 to write a book report on _Harry Potter and the Sorcerer's Stone_, you cannot insert the entire book contents in the prompt. GPT-4's token limit is either 8k or 32k, depending on the model variant you're using. This means you need to insert the book in chunks, and then stitch the chunks together. Figuring out an effective way to do this is a common source of errors.

Now, let's see why focusing on context can help us evaluate our LLM-backed app.

## What to evaluate? Context!

![agent context](/img/posts/rag_eval/agent_context.png)
<div class="caption">In <em><a href="https://www.lesswrong.com/posts/566kBoPi76t8KAkoD/on-autogpt">On AutoGPT</a></em>, Zvi shows how a lot of the work in creating an autonomous agent is context-related.</div>

> How important evals are to the team is a major differentiator between folks rushing out hot garbage and those seriously building products in the space.
<div class="caption">From a <a href="https://news.ycombinator.com/item?id=36789901">Hacker News Comment</a>, the source-of-truth on the Internet.</div>

My first attempt at automating the evaluation of my LLM-backed app tested the entire question and answer flow, providing static ground truth natural language answers to questions. While testing the entire flow gives a complete picture of how the app performs, it encourages anti-patterns:

1. I run the test suite less frequently and add fewer tests as its slow to run.
2. I add fewer tests as its slow to human evaluate natural language responses.
3. I add unrealistic questions and ground truths to get more consistent results as my environment the agent interacts with (AWS resources and monitoring data) is constantly changing and the correct answers change with it.

![wait_meme](/img/posts/rag_eval/wait_meme.jpg)

My first attempt at evaluation was very similar to how traditional software developers perform integration testing. __Integration testing evaluates the entire app__, but is __slow to run__ and __slow to human evaluate.__

__To implement EDD (or TDD)__, it's __critical that the evaluation/test suite is fast__ to run. Otherwise the code <-> eval feedback loop is painfully slow and you stop running evals. Rather than testing the entire question and answer flow, I changed my evaluation efforts to focus on the most common source of errors: context.

My initial human-evaluation, integration-like eval suite took about 30 minutes. The new system runs in the background (and takes just a couple of minutes), and scores the results in nearly identical fashion:

![human vs model eval](/img/posts/rag_eval/human_model_eval.png)
<div class="caption">The new system scores results in nearly identical fashion to human evaluation and is fast to run. In the above screenshot, "accuracy" is the human eval result and "confidence" is the model-eval result.</div>

Now I'll get into the tradeoffs I made to get to make this faster evaluation system.

## EDD speed over accuracy tradeoffs

I needed to accept some tradeoffs to experience a fast code <-> eval feedback loop: I can't test the entire system and I can't rely on human evaluation. Below are the two key tradeoffs I made in the interest of speed.

### 1. Model-based eval over human

If you're evaluating an LLM-backed app, you are very likely evaluating natural language responses. Natural Language Processing (NLP) is a classical machine learning domain and these models have their own evaluation techniques and metrics like BLEU, ROUGE, BERTScore, and MoverScore.

However, there’s poor correlation between these NLP evaluation metrics and human judgments. From Eugene Yan's excellent post _[Patterns for Building LLM-based Systems & Products](https://eugeneyan.com/writing/llm-patterns/)_:

> BLEU, ROUGE, and others have had negative correlation with how humans evaluate fluency. They also showed moderate to less correlation with human adequacy scores. In particular, BLEU and ROUGE have low correlation with tasks that require creativity and diversity.

I already mentioned that human evaluation is slow. It's also expensive. Could an LLM fill in as a human evaluator?

It turns out, LLMs are good substitutes for human evaluators:

> GPT-4 as an evaluator had a high Spearman correlation with human judgments (0.514), outperforming all previous methods. It also outperformed traditional metrics on aspects such as coherence, consistency, fluency, and relevance. On topical chat, it did better than traditional metrics such as ROUGE-L, BLEU-4, and BERTScore across several criteria such as naturalness, coherence, engagingness, and groundedness.

And:

> Overall, they found that GPT-4 not only provided consistent scores but could also give detailed explanations for those scores. Under the single answer grading paradigm, GPT-4 had higher agreement with humans (85%) than the humans had amongst themselves (81%). This suggests that GPT-4’s judgment aligns closely with the human evaluators.

For a fast development flow, I'm OK with the tradeoff of using an LLM to evaluate my app.

### 2. Dynamic ground truth over static

__It's critical that the results we're evaluating are as close to the end user experience as possible.__ For example, if you have a traditional web app that is most frequently used on mobile devices, it's likely your test suite for the mobile experience is more extensive than the desktop version.

LLM-backed apps - especially autonomous agents - are often deployed in environments where the underlying data is changing frequently. My autonomous agent, [OpsTower.ai](https://github.com/opstower-ai/llm-opstower), interacts with AWS to retrieve real-time data about a customer's cloud infrastructure. The data is constantly changing and there are multiple ways to fetch information that can lead to the same result. It's not feasible to build a static test suite or mock all possible API calls that cover generated code for API interactions.

For example, if our dataset is a Pandas dataframe of Titantic passengers like this:

```python
titanic_path = "https://raw.githubusercontent.com/jorisvandenbossche/pandas-tutorial/master/data/titanic.csv"
df = pd.read_csv(titanic_path)
```

...and we want to dynamically validate the number of passengers on the Titanic, we can reference a function that has been verified by a human to return the correct answer:

```
("How many passengers were on the Titanic?", "len(df)"),
```

As of this writing, the closest approach I've seen to this in a product or project is Langsmith's [Dynamic Evaluation](https://github.com/langchain-ai/langsmith-cookbook/blob/main/testing-examples/dynamic-data/testing_dynamic_data.ipynb) (closed beta). However, it's not difficult to implement on your own.

## Implementing my streamlined EDD flow

There were two key parts I needed to implement to streamline my EDD flow:

1. Creating a dynamic ground truth dataset.
2. Implementing an LLM-based eval.

### Creating a dynamic ground truth dataset

#### 1. Generate dataset questions (can use an LLM to assist)

To start, I use ChatGPT to generate a few initial questions for a new evaluation dataset. I'm working on [OpsTower.ai](https://github.com/opstower/llm-opstower), a DevOps AI Assistant, so lets create a dataset of questions about AWS CloudWatch Logs. [Here](https://chat.openai.com/share/8ca6e589-e0f0-4c70-a7a7-e96c0b5d76fb) is my transcript.

![cloudwatch logs](/img/posts/rag_eval/cloudwatch_logs_eval.png)
<div class="caption">I use ChatGPT to generate dataset questions. This is for an AWS Cloudwatch Logs dataset to test <a href="https://github.com/opstower/llm-opstower">OpsTower.ai</a>.</div>

I then paste these questions in a `aws_cloudwatch_logs.csv` file:

<script src="https://gist.github.com/itsderek23/be29a12b3b565a7918004807e440d387.js"></script>

#### 2. Programmatically generate responses for each question

Next I programmatically generate responses to each of these questions using the current AI agent. For my app, it looks like this:

```ruby
demo_source = Eval::VendorTest.new.source.save!
test = AgentTest.create!(source: demo_source, dataset_file: "aws_cloudwatch_logs.csv")
test.run!
```
When the test completes, I view the answers in the UI:

<a href ="/img/posts/rag_eval/aws_cloudwatch_log_results.png" target="_blank"><img src="/img/posts/rag_eval/aws_cloudwatch_logs_results.png"></a>
<div class="caption">Above is a screenshot of the custom evaluation results page within my app. It provides summary stats and details on each question-answer pair.</div>

#### 3. Save ground truth functions for generating context

I then review the results. Results generally fall into three buckets:

1. Works as-is - the agent generated valid context and answered the question correctly. I'll reuse the functions that generated the context as ground truth.
2. Hybrid - the agent did not generate valid context, but the code it created to generate context is a solid starting point. I can modify the code then save it as a reference function.
3. Fully human-generated - the agent failed miserably at code generation. I'll write a new function from scratch.

I like to go from easy to hard. The easiest ones are "works as-is" as I can simply copy and save the generated code.

To create a reference function from a "works as-is" result, I generate a saved method from the code the agent generated then reference that function by ID in the `aws_cloudwatch_logs.csv` file.

For example, the question _"How many CloudWatch Log Groups do I have?"_ is correctly answered below:

<a href ="/img/posts/rag_eval/how_many_logs_chat.png" target="_blank"><img src="/img/posts/rag_eval/how_many_logs_chat.png"></a>

I want to save the code the LLM generated, starting with the `get_cloudwatch_log_groups_count` method. In my app, I can do this by executing:

```ruby
saved_methods = SavedMethod.create_from_chat!("482273fb-4c79-4cd2-bc4d-382945c38e42")
saved_methods.map(&:id)
["74294dde-8cd0-4803-a0c2-7c117b8b15de"]
```

I then paste the saved method ID into the `aws_cloudwatch_logs.csv` file:

```csv
question,reference_functions
How many CloudWatch Log Groups do I have?,"74294dde-8cd0-4803-a0c2-7c117b8b15de"
```

I repeat this process for each "works as-is" result. Hybrid and fully human-generated results are handled similarly, but with more changes to the code.

#### 4. Run evaluation.

After adding reference functions, I run an evaluation. I can see that in the latest run, the agent incorrectly answered the question _"How many CloudWatch Log Groups do I have?"_:

<a href ="/img/posts/rag_eval/incorrect.png" target="_blank"><img src="/img/posts/rag_eval/incorrect.png"></a>
<div class="caption">This shows the evaluation result on a single question-answer pair. In this case, this did not pass evaluation.</div>


### LLM-based eval

It's not possible to have a quick code change - eval feedback loop if you are relying on human evaluation. We need another approach that leverages LLM prompts to do the eval.

#### Prompt 1: Answer from a reference function

The first prompt is used to generate the ground truth answer from the referenced function we saved earlier. It looks like this:

```
You are given a question and function output(s). Return a final answer to the question based on the function output. BE SPECIFIC, sharing data to back up your answer when you can.

Question:: %%QUESTION%%

# Function Output(s)
%%FUNCTION_OUTPUTS%%

Begin!
```
#### Prompt 2: Evaluation Prompt

The second prompt is my model-evaluation prompt. This prompt generates a confidence score, comparing the answer from the agent vs the response generated from the above prompt:

```
I'm comparing the output of two different systems. I need you to compare the two responses to the same question and see if they are functionally equivalent.

It's ok if one response is more detailed than the other, as long as they are functionally equivalent.
It's ok if when comparing floats (ie 10.5 or 12.4%), if the two numbers are within 30% of each other.

Question: %%QUESTION%%

# Response

%%RESPONSE_TO_EVAL%%

# Reference Response

%%REFERENCE_RESPONSE%%

# Response format

Answer in the following format:

{
  "confidence": Return a confidence value btw 0.0-1.0 that the responses are functionally equivalent,
  "why": "A single sentence explaining why you chose this confidence value"
}

Begin!
```

### Rinse and repeat

Once I'm getting acceptable accuracy on an evaluation dataset (ex: 80% or greater), I'll repeat the process outlined above:

1. Add questions with ChatGPT.
2. Add reference functions.
3. Run evaluation, tweak the app, run evaluation, etc.

## Conclusion

I've dramatically increased the accuracy, capabilities, and reliability of my LLM-backed app by leveraging a more automated, streamlined form of Eval Driven Development. My flavor of EDD leans on the LLM to generate the question dataset, uses human-evaluated reference functions to generate context, re-assembles ground truth answers via the LLM, and finally uses the LLM again to simulate human evaluation.

For my app, the resulting confidence scores of this approach are typically within 5% of human-eval scoring at a fraction of the time spent.

### EDD Resources

To see example datasets, reference functions, and evaluation prompts, checkout the [DevOps AI Assistant Open Leaderboard](https://github.com/opstower-ai/devops-ai-open-leaderboard) on GitHub.

See [Awesome Eval Driven Development](https://github.com/itsderek23/awesome-eval-driven-development) on GitHub a continually updated set of resources related to Eval Driven Development.




