---
layout: post
title:  "Streamlining RAG evaluation"
date: 2023-10-04 05:00:00 -0600
---

![llm product curve](/img/posts/rag_eval/llm_product_curve.png)

My first weeks working with GPT-4 were _magical_. I was doing things that I previously thought were _impossible_. However, as it went from promising proof-of-concept to something I wanted to share, I fell into a trough of sorrow. Every live demo felt like a YOLO moment ... who knew what would happen?

My app - a [DevOps AI Assistant](https://github.com/opstower-ai/llm-opstower) called OpsTower.ai - could perform a few mic-drop tasks, but getting it to _reliably reproduce_ those results was a frustrating, non-deterministic nightmare. A public release was perpetually one week away.

Fast forward to today: OpsTower.ai is State of the Art (SOTA) in the three categories it competes in on the [DevOps AI Assistant Open Leaderboard](https://github.com/opstower-ai/devops-ai-open-leaderboard).

<img src="/img/posts/rag_eval/leaderboard.png" width="500px">
<div class="caption">The DevOps AI Assistant Open Leaderboard is a set evaluation datasets for AWS Services, AWS Cloudwatch metrics, AWS Billing, and kubectl commands. The evaluation procedure is open-source and available on GitHub.</div>

In this post, I share how I emerged from my AI trough of sorrow via a streamlined form of [Eval Driven Development (EDD)](https://eugeneyan.com/writing/llm-patterns/#how-to-apply-evals).

## Table of Contents

1. [What is Eval Driven Development (EDD)?](#what-is-eval-driven-development-edd)
1. [How to make EDD fast?](#how-to-make-edd-fast)
1. [Context adds knowledge to LLMs](#context-adds-knowledge-to-llms)
1. [Poor context leads to poor results](#poor-context-leads-to-poor-results)
1. [EDD speed over accuracy tradeoffs](#edd-speed-over-accuracy-tradeoffs)
1. [Implementing my streamlined EDD flow](#implementing-my-streamlined-edd-flow)
1. [Conclusion](#conclusion)
1. [EDD Resources](#edd-resources)

## What is Eval Driven Development (EDD)?

I first saw this term in Eugene Yan's seminal blog post [_Patterns for Building LLM-based Systems & Products_](https://eugeneyan.com/writing/llm-patterns). I define EDD as:

> Eval Driven Development (EDD) is a process that uses an evaluation suite to guide which levers (prompt, context, model params) to pull (and how far) to improve accuracy.

### How does EDD compare to ML evaluation and Test Driven Development (TDD)?

EDD combines elements of machine learning model evaluation and software Test Driven Development (TDD). In the table below, I've summarized how model eval and TDD compare across several aspects. I indicate which approach is most applicable to EDD via the __✅&nbsp;EDD__ label:

| Aspect                | ML Evaluation                                         | TDD                         |
|-----------------------|-------------------------------------------------------|-----------------------------|
| **Nature**            | Experimental: involves preprocessing, training, and tuning. | Deterministic: involves writing tests, then the code, and getting immediate feedback. __✅&nbsp;EDD__ |
| **Feedback Type**     | Probabilistic: results can vary with slight changes. __✅&nbsp;EDD__  | Deterministic: code either passes or fails the test. |
| **Duration**          | Can be long, especially with large datasets or complex models. | Typically short, as unit tests are designed to be quick and focused. __✅&nbsp;EDD__ |
| **Infrastructure**    | Requires significant computational resources for complex models. | Minimal resources needed for most tests. __✅&nbsp;EDD__|
| **Evaluation**        | Might involve multiple metrics and can be context-dependent. __✅&nbsp;EDD__| Immediate and binary: pass or fail. |
| **Tooling**           | Advanced platforms available for prototyping but can be resource-intensive. | Wide range of tools for rapid development and continuous integration. __✅&nbsp;EDD__|
| **Determinism**       | Results can vary between runs; uncertainty is inherent. __✅&nbsp;EDD__| Results are consistent; code behavior is expected to be deterministic. |

Summarizing the key elements: __The EDD feedback cycle should be fast__ (like TDD) so you can quickly iterate on prompts, context retrieval, and model parameters. __The evaluation is probabilistic__ (like ML evaluation) as you're evaluating an LLM-backed app, not a deterministic system.

## How to make EDD fast?

By trading speed for rigor! Below is a diagram that shows the number of tests that are typically a part of a software project. At the top is manual QA testing (slow, but high rigor) and at the bottom is unit testing (the bulk of tests, focused on small parts).

![testing pyramid](/img/posts/rag_eval/testing_pyramid.jpg)
<div class="caption">From <a href="https://www.alexhyett.com/unit-testing-vs-integration-testing/">
<em>Unit Testing vs Integration Testing - Key Differences</em>
</a> by Alex Hyett</div>

A developer practicing TDD is not doing QA testing after every code change even though these tests give a complete picture of a system. QA testing is too slow and doesn't provide as much coverage as programmatic tests. However, developers _are_ running unit tests after code changes.

A key part of building fast unit tests is choosing what not to test. You don't need to test every part of your system. You need to test the most common source of errors.

The same should be true for EDD. We need to focus on the __most common source of LLM app errors__ and test that. For LLM-backed apps, that's __poor context__.

## Context adds knowledge to LLMs

What is _context_? Context is external information you insert into an LLM prompt that the LLM references when generating a response. Context is the easiest way to knowledge to an LLM.

Context is commonly inserted via some form of [Retrieval Augmented Generation (RAG)](https://www.promptingguide.ai/techniques/rag). A RAG app fetches up-to-date or semantically-relevant text from an external source and inserts it into an LLM prompt as _context_.

![context_example](/img/posts/rag_eval/context_example.png)
<div class="caption">Context is just text. In the example above, I've provided instructions that the model should reference when responding in the OpenAI Playground. You can insert this text programmatically in many ways.</div>

For example, if you are building a "chat with your docs" app, you could load the doc into a vector database, query the database for the snippets that appear most relevant to your question, and insert those snippets as context into the LLM prompt. The LLM then generate a response based on the question and the context you inserted.

While the most common RAG apps fetch semantically-relevant snippets from vector databases, there are many other services you could interact with, parse results, and insert the results as context. For example, you could interact with a weather REST API to get the forecast for a location in JSON format, insert that JSON into a prompt, and let the LLM tell you the weather in natural language:

![api context example](/img/posts/rag_eval/api_context_example.png)
<div class="caption">In this example, I insert the JSON response for the weather forecast as context into the LLM prompt. GPT-4 is able to understand raw data.</div>

When people build apps that interact with external systems beyond vector databases, they typically refer to these as _autonomous agents_. Autonomous agents - in almost all cases - have a RAG foundation coupled with extra reasoning sauce. Interacting with diverse external systems often requires additional handling of the results (for example: handling HTTP errors, code generation errors, etc).

So, __the common piece of an LLM-backed app__ (a RAG system or an autonomous agent) is that you __insert context__ into the prompt and ask the LLM to reference it when providing a response.

## Poor context leads to poor results

![agent context](/img/posts/rag_eval/agent_context.png)
<div class="caption">In <em><a href="https://www.lesswrong.com/posts/566kBoPi76t8KAkoD/on-autogpt">On AutoGPT</a></em>, Zvi shows how a lot of the work in creating an autonomous agent is context-related.</div>

Let's examine how poor context impacts accuracy and common causes of poor context. Here's a [slide](https://drive.google.com/file/d/1dB-RQhZC_Q1iAsHkNNdkqtxxXqYODFYy/view) from Colin Jarvis of OpenAI with a matrix of typical RAG evaluation results:

![moat](/img/posts/rag_eval/moat.png)
<div class="caption">In this example, only 5% of answers are incorrect when the retrieval is correct.</div>

__Colin shows that incorrect retrieval is responsible for 4x the number of incorrect answers vs. other sources.__ While there are no guarantees with LLMs, the surest one I've found for getting an inaccurate response: feed the LLM bad context. For example, if you ask for the weather _today_ but you actually insert the forecast for _tomorrow_ into the context, the LLM will not magically change your context and fetch the weather for the correct date. The LLM is reliable when provided with good context.

_My experience of poor context being the major driver of poor accuracy echos Colin's._

How hard can it be to dump context into a prompt correctly? Well, it turns out there are many ways to feed an LLM bad context.

### Common causes of poor context

There are many types of context that are likely to trigger an incorrect response, but some of the most common are:

#### 1. Irrelevant context

If your RAG system is injecting poor context into a prompt, the LLM will have a hard time generating a good and consistent response. Poor context covers _many_ categories: unrelated context, too much context, expired results, unexpected context, etc. There are [many ways to tune this](https://medium.com/@imicknl/how-to-improve-your-chatgpt-on-your-data-solution-d1e842d87404).

#### 2. Errors due to working around the context window limit

If you wanted GPT-4 to write a book report on _Harry Potter and the Sorcerer's Stone_, you cannot insert the entire book contents in the prompt. GPT-4's token limit is between 8k or 32k, depending on the model variant you're using. This means you need to insert the book in chunks, and then stitch the chunks together. Figuring out an effective way to do this is a common source of errors.

#### 3. Code generation inconsistencies

![code generation](/img/posts/rag_eval/code_int.png)
<div class="caption">ChatGPT's Advanced Data Analysis extension generates code, evaluates it, and returns a natural language answer based on the code result. Agents like this generate widely different context that can trigger unexpected results.</div>

Autonomous agents that rely on code generation and evaluation to interact with an external system (like a weather forecasting API) typically insert the result as context. These apps are hit from two sides. First - unlike querying a Vector DB to insert context - you aren't reasonably guaranteed that the code the LLM generated will execute without error. Second, the result format may not be understandable when inserted as context. Perhaps the most well-known autonomous agent that relies on code generation and evaluation is ChatGPT's [Advanced Data Analysis](https://dev.to/ppiova/advanced-data-analysis-in-chatgpt-replaces-code-interpreter-3lil) BETA feature. If you try this feature, you'll likely see it generate and evaluate code, encounter errors, self-recover, and parse varying formats of data as context.

Now, let's see why focusing on context can speed up EDD.

## EDD speed over accuracy tradeoffs

Just like unit testing doesn't touch every part of a system - which would be slow - EDD doesn't need to test every part of an LLM-backed app. I'll bet that "good enough" accuracy and a fast feedback loop results in a more reliable app.

Below are the two key tradeoffs I made in the interest of EDD speed.

### 1. Dynamic ground truth over static

__It's critical that the results we're evaluating are as close to the end user experience as possible.__ For example, if you have a traditional web app that is most frequently used on mobile devices, it's likely your test suite for the mobile experience is more extensive than the desktop version.

LLM-backed apps - especially autonomous agents - are often deployed in environments where the underlying data is changing frequently. My autonomous agent, [OpsTower.ai](https://github.com/opstower-ai/llm-opstower), interacts with AWS to retrieve real-time data about a customer's cloud infrastructure. The data is constantly changing and there are multiple ways to fetch information that can lead to the same result. It's not feasible to build a static test suite or mock all possible API calls that cover generated code for API interactions.

For example, if our dataset is a Pandas dataframe of Titantic passengers like this:

```python
titanic_path = "https://raw.githubusercontent.com/jorisvandenbossche/pandas-tutorial/master/data/titanic.csv"
df = pd.read_csv(titanic_path)
```

...and we want to dynamically validate the number of passengers on the Titanic, we can reference a function that has been verified by a human to return the correct answer:

```
("How many passengers were on the Titanic?", "len(df)"),
```

The result of `len(df)` is inserted as context into the LLM prompt. As covered in the _Poor context leads to poor results_ section, we can be reasonably sure that the LLM will generate a correct answer given this correct context.

As of this writing, the closest approach I've seen to this in a product or project is Langsmith's [Dynamic Evaluation](https://github.com/langchain-ai/langsmith-cookbook/blob/main/testing-examples/dynamic-data/testing_dynamic_data.ipynb) (closed beta). I'll show how I've implemented dynamic eval in a bit.

### 2. Model-based eval over human

If you're evaluating an LLM-backed app, you are very likely evaluating natural language responses. Natural Language Processing (NLP) is a classical machine learning domain and these models have their own evaluation techniques and metrics like BLEU, ROUGE, BERTScore, and MoverScore.

However, there’s poor correlation between these NLP evaluation metrics and human judgments. From Eugene Yan's excellent post _[Patterns for Building LLM-based Systems & Products](https://eugeneyan.com/writing/llm-patterns/)_:

> BLEU, ROUGE, and others have had negative correlation with how humans evaluate fluency. They also showed moderate to less correlation with human adequacy scores. In particular, BLEU and ROUGE have low correlation with tasks that require creativity and diversity.

I already mentioned that human evaluation is slow. It's also expensive. Could an LLM fill in as a human evaluator?

It turns out, LLMs are good substitutes for human evaluators:

> GPT-4 as an evaluator had a high Spearman correlation with human judgments (0.514), outperforming all previous methods. It also outperformed traditional metrics on aspects such as coherence, consistency, fluency, and relevance. On topical chat, it did better than traditional metrics such as ROUGE-L, BLEU-4, and BERTScore across several criteria such as naturalness, coherence, engagingness, and groundedness.

And:

> Overall, they found that GPT-4 not only provided consistent scores but could also give detailed explanations for those scores. Under the single answer grading paradigm, GPT-4 had higher agreement with humans (85%) than the humans had amongst themselves (81%). This suggests that GPT-4’s judgment aligns closely with the human evaluators.

For a fast development flow, I'm OK with the tradeoff of using an LLM to evaluate my app.

## Implementing my streamlined EDD flow

There were two key parts I needed to implement to streamline my EDD flow:

1. Creating a dynamic ground truth dataset.
2. Implementing an LLM-based eval.

### Creating a dynamic ground truth dataset

#### 1. Generate dataset questions (can use an LLM to assist)

To start, I use ChatGPT to generate a few initial questions for a new evaluation dataset. I'm working on [OpsTower.ai](https://github.com/opstower/llm-opstower), a DevOps AI Assistant, so lets create a dataset of questions about AWS CloudWatch Logs. [Here](https://chat.openai.com/share/8ca6e589-e0f0-4c70-a7a7-e96c0b5d76fb) is my transcript.

![cloudwatch logs](/img/posts/rag_eval/cloudwatch_logs_eval.png)
<div class="caption">I use ChatGPT to generate dataset questions. This is for an AWS Cloudwatch Logs dataset to test <a href="https://github.com/opstower/llm-opstower">OpsTower.ai</a>.</div>

I then paste these questions in a `aws_cloudwatch_logs.csv` file:

<script src="https://gist.github.com/itsderek23/be29a12b3b565a7918004807e440d387.js"></script>

#### 2. Programmatically generate responses for each question

Next I programmatically generate responses to each of these questions using the current AI agent. For my app, it looks like this:

```ruby
demo_source = Eval::VendorTest.new.source.save!
test = AgentTest.create!(source: demo_source, dataset_file: "aws_cloudwatch_logs.csv")
test.run!
```
When the test completes, I view the answers in the UI:

<a href ="/img/posts/rag_eval/aws_cloudwatch_log_results.png" target="_blank"><img src="/img/posts/rag_eval/aws_cloudwatch_logs_results.png"></a>
<div class="caption">Above is a screenshot of the custom evaluation results page within my app. It provides summary stats and details on each question-answer pair.</div>

#### 3. Save ground truth functions for generating context

I then review the results. Results generally fall into three buckets:

1. Works as-is - the agent generated valid context and answered the question correctly. I'll reuse the functions that generated the context as ground truth.
2. Hybrid - the agent did not generate valid context, but the code it created to generate context is a solid starting point. I can modify the code then save it as a reference function.
3. Fully human-generated - the agent failed miserably at code generation. I'll write a new function from scratch.

I like to go from easy to hard. The easiest ones are "works as-is" as I can simply copy and save the generated code.

To create a reference function from a "works as-is" result, I generate a saved method from the code the agent generated then reference that function by ID in the `aws_cloudwatch_logs.csv` file.

For example, the question _"How many CloudWatch Log Groups do I have?"_ is correctly answered below:

<a href ="/img/posts/rag_eval/how_many_logs_chat.png" target="_blank"><img src="/img/posts/rag_eval/how_many_logs_chat.png"></a>

I want to save the code the LLM generated, starting with the `get_cloudwatch_log_groups_count` method. In my app, I can do this by executing:

```ruby
saved_methods = SavedMethod.create_from_chat!("482273fb-4c79-4cd2-bc4d-382945c38e42")
saved_methods.map(&:id)
["74294dde-8cd0-4803-a0c2-7c117b8b15de"]
```

I then paste the saved method ID into the `aws_cloudwatch_logs.csv` file:

<script src="https://gist.github.com/itsderek23/3faa7f4f7e677362e3eab7f87cec617b.js"></script>

I repeat this process for each "works as-is" result. Hybrid and fully human-generated results are handled similarly, but with more changes to the code.

#### 4. Run evaluation.

After adding reference functions, I run an evaluation. I can see that in the latest run, the agent incorrectly answered the question _"How many CloudWatch Log Groups do I have?"_:

<a href ="/img/posts/rag_eval/incorrect.png" target="_blank"><img src="/img/posts/rag_eval/incorrect.png"></a>
<div class="caption">This shows the evaluation result on a single question-answer pair. In this case, the result did not pass evaluation.</div>


### LLM-based eval

It's not possible to have a quick code <-> eval feedback loop if you are relying on human evaluation. We need another approach that leverages LLM prompts to do the eval.

#### Prompt 1: Answer from a reference function

The first prompt is used to generate the ground truth answer from the referenced function we saved earlier. It looks like this:

<script src="https://gist.github.com/itsderek23/b74956714d8f3303c9e77de89e98a707.js"></script>

#### Prompt 2: Evaluation Prompt

The second prompt is my model-evaluation prompt. This prompt generates a confidence score, comparing the answer from the agent vs the response generated from the above prompt:

<script src="https://gist.github.com/itsderek23/8a1fde245a24889b68b521cdf3e0f09a.js"></script>

### Rinse and repeat

Once I'm getting acceptable accuracy on an evaluation dataset (ex: 80% or greater), I'll repeat the process outlined above:

1. Add questions with ChatGPT.
2. Add reference functions.
3. Run evaluation, tweak the app, run evaluation, etc.

## Conclusion

Humans be-damned, I've gone all-in on the LLM.

I've dramatically increased the accuracy, capabilities, and reliability of my LLM-backed app by leveraging a more automated, streamlined form of Eval Driven Development (EDD). My flavor of EDD leans on the LLM to generate the question dataset, uses human-evaluated reference functions to generate context, re-assembles ground truth answers via the LLM, and finally uses the LLM again to simulate human evaluation.

For my app, the resulting confidence scores of this approach are typically within 5% of human-eval scoring at a fraction of the time spent.

### EDD Resources

To see example datasets, reference functions, and evaluation prompts, checkout the [DevOps AI Assistant Open Leaderboard](https://github.com/opstower-ai/devops-ai-open-leaderboard) on GitHub.

See [Awesome Eval Driven Development](https://github.com/itsderek23/awesome-eval-driven-development) on GitHub a continually updated set of resources related to Eval Driven Development.




